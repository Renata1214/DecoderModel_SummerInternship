{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676874d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from validator import Validator\n",
    "import csv \n",
    "from torch.optim.lr_scheduler import StepLR  # or use ReduceLROnPlateau, etc.\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # Replace 1 with the GPU index you want\n",
    "\n",
    "print(torch.cuda.get_device_name(0))  # This will now refer to GPU 1 as \"GPU 0\" in the notebook context\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current GPU:\", torch.cuda.current_device())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f92d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING WITH HEX TO BYTE \n",
    "\n",
    "# BOS = Beginning of Sequence → marks the start of a Modbus message.\n",
    "# SEP = Separator → marks the boundary between the query and the response.\n",
    "# EOS = End of Sequence → marks the end of the full sequence.\n",
    "# PAD = Padding token → used to pad shorter sequences so they fit in a batch.\n",
    "# VOCAB_SIZE = 260 → bytes (0–255) + 4 special tokens (256–259)\n",
    "\n",
    "# Constants\n",
    "BOS, SEP, EOS, PAD = 256, 257, 258, 259\n",
    "VOCAB_SIZE = 260\n",
    "\n",
    "# Load and preprocess dataset\n",
    "#hexadecimal string into a list of byte values\n",
    "def hex_to_bytes(hex_str):\n",
    "    # 2-character group is treated as a hex byte and converted to decimal\n",
    "    return [int(hex_str[i:i+2], 16) for i in range(0, len(hex_str), 2)]\n",
    "\n",
    "def preprocess(example):\n",
    "    q = hex_to_bytes(example[\"query\"])\n",
    "    r = hex_to_bytes(example[\"response\"])\n",
    "    \n",
    "    input_ids = [BOS] + q + [SEP] + r + [EOS]\n",
    "    sep_index = len(q) + 1  # BOS + query\n",
    "    label_start = sep_index + 1\n",
    "    labels = [-100] * label_start + input_ids[label_start:]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "with open(\"modbus_dataset.jsonl\", \"r\") as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"modbus_dataset_test.jsonl\", \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"modbus_dataset_validation.jsonl\", \"r\") as f:\n",
    "    validation_data = [json.loads(line) for line in f]\n",
    "\n",
    "#turns a list of dictionaries into a Dataset object\n",
    "train_dataset = Dataset.from_list([preprocess(d) for d in train_data])\n",
    "test_dataset = Dataset.from_list([preprocess(d) for d in test_data])\n",
    "val_dataset = Dataset.from_list([preprocess(d) for d in validation_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e830b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model WITHOUT WEIGHT INITIALIZATION\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    # vocab_size = how many distinct tokens we can embed\n",
    "    #d_model =  size of each embedding vector.\n",
    "    #n_heads: number of attention heads. Helps the model focus on different parts of the sequence simultaneously.\n",
    "    # n_layers: number of transformer layers stacked\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        #Converts each token ID into a dense vector of dimension d_model.\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD)\n",
    "        self.pos_emb = nn.Embedding(512, d_model)\n",
    "        # Even though we call it \"encoder layer\", apply causal masks later to make it autoregressive like a decoder.\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4, dropout=dropout, batch_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ]) # dim_feedforward = Size of the hidden layer inside the feed-forward network of the Transformer.\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        #Maps the output of the model (of shape [batch, seq_len, d_model]) to logits over the vocabulary.\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size() # B is the batch size, T is the sequence length (number of tokens in each input)\n",
    "        positions = torch.arange(0, T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.token_emb(x) + self.pos_emb(positions)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(T).to(x.device) # Ensures the model can only attend to current and past tokens, not future ones\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=tgt_mask)  # Applies each encoder layer sequentially\n",
    "        x = self.norm(x)\n",
    "        return self.output(x) # Projects the final hidden states back to vocabulary space: for each token position, the model predicts a probability distribution over the 260 tokens (0–259)\n",
    "\n",
    "# Collate\n",
    "def collate(batch):\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    padded_inputs = [x[\"input_ids\"] + [PAD] * (max_len - len(x[\"input_ids\"])) for x in batch]\n",
    "    padded_labels = [x[\"labels\"] + [-100] * (max_len - len(x[\"labels\"])) for x in batch]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(padded_inputs, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(padded_labels, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# Generate\n",
    "def generate(model, input_seq, max_len=32):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        #Copies the input sequence so the original is not modified\n",
    "        seq = input_seq[:]\n",
    "        for _ in range(max_len):\n",
    "            x = torch.tensor([seq], dtype=torch.long).to(next(model.parameters()).device)\n",
    "            #print(\"Whatever is X: \", x)\n",
    "            logits = model(x)\n",
    "            next_token = logits[0, -1].argmax().item()\n",
    "\n",
    "            if next_token == EOS:\n",
    "                break\n",
    "            seq.append(next_token)\n",
    "        return seq\n",
    "\n",
    "# Evaluate\n",
    "def evaluate(model, dataset, device, print_limit=5):\n",
    "    correct, total, shown = 0, 0, 0\n",
    "    for item in dataset:\n",
    "        tokens = item[\"input_ids\"]\n",
    "        sep_idx = tokens.index(SEP)\n",
    "        query = tokens[:sep_idx+1]\n",
    "        true = tokens[sep_idx+1:-1]\n",
    "\n",
    "        pred = generate(model, query)\n",
    "        pred = pred[pred.index(SEP)+1:]\n",
    "        if EOS in pred:\n",
    "            pred = pred[:pred.index(EOS)]\n",
    "\n",
    "        matches = sum(1 for i in range(min(len(true), len(pred))) if true[i] == pred[i])\n",
    "        correct += matches\n",
    "        total += len(true)\n",
    "\n",
    "        if shown < print_limit:\n",
    "            def tohex(x): return ''.join(f\"{b:02x}\" for b in x)\n",
    "            print(\"\\nQuery:             \", tohex(query[1:-1]))\n",
    "            print(\"True Response:     \", tohex(true))\n",
    "            print(\"Predicted Response:\", tohex(pred))\n",
    "            shown += 1\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"\\n✅ Byte-Level Accuracy: {acc:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "def evaluate_with_validator(model, dataset, device, print_limit=5, end_address=3, save_errors=True, error_log_file=\"validator_failures.csv\"):\n",
    "    exact_matches = 0\n",
    "    total_samples = 0\n",
    "    shown = 0\n",
    "    failed_validations = 0\n",
    "    failed_cases = []\n",
    "\n",
    "    is_byte_level = VOCAB_SIZE > 100\n",
    "    id_to_char = {i: c for i, c in enumerate(\"0123456789abcdef\")}\n",
    "    all_logged_predictions = []\n",
    "\n",
    "\n",
    "    for item in dataset:\n",
    "        tokens = item[\"input_ids\"]\n",
    "        sep_idx = tokens.index(SEP)\n",
    "        query = tokens[:sep_idx+1]\n",
    "        true = tokens[sep_idx+1:-1]\n",
    "        #print(\"Query is : \", query)\n",
    "        pred = generate(model, query)\n",
    "        #print(\"Prediction is: \", pred)\n",
    "\n",
    "        pred = pred[pred.index(SEP)+1:]\n",
    "        if EOS in pred:\n",
    "            pred = pred[:pred.index(EOS)]\n",
    "\n",
    "        if is_byte_level:\n",
    "            q_hex = ''.join(f\"{b:02x}\" for b in query[1:-1])\n",
    "            r_true_hex = ''.join(f\"{b:02x}\" for b in true)\n",
    "            r_pred_hex = ''.join(f\"{b:02x}\" for b in pred)\n",
    "        else:\n",
    "            q_hex = ''.join(id_to_char[b] for b in query[1:-1])\n",
    "            r_true_hex = ''.join(id_to_char[b] for b in true)\n",
    "            r_pred_hex = ''.join(id_to_char[b] for b in pred)\n",
    "\n",
    "        if pred == true:\n",
    "            exact_matches += 1\n",
    "        total_samples += 1\n",
    "\n",
    "        all_logged_predictions.append({\n",
    "        \"query\": q_hex,\n",
    "        \"expected_response\": r_true_hex,\n",
    "        \"predicted_response\": r_pred_hex,\n",
    "        \"is_exact_match\": pred == true})\n",
    "\n",
    "\n",
    "        try:\n",
    "            val = Validator(q_hex, r_pred_hex, r_true_hex, end_address)\n",
    "            val.check_header_ids()\n",
    "            val.check_payload()\n",
    "        except Exception as e:\n",
    "            failed_validations += 1\n",
    "            failed_cases.append({\n",
    "                \"query\": q_hex,\n",
    "                \"expected_response\": r_true_hex,\n",
    "                \"predicted_response\": r_pred_hex,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "            if shown < print_limit:\n",
    "                print(f\"\\n❌ Validator Error: {e}\")\n",
    "\n",
    "        if shown < print_limit:\n",
    "            print(\"\\nQuery:             \", q_hex)\n",
    "            print(\"True Response:     \", r_true_hex)\n",
    "            print(\"Predicted Response:\", r_pred_hex)\n",
    "            shown += 1\n",
    "\n",
    "    acc = 100 * exact_matches / total_samples if total_samples else 0\n",
    "    val_rate = 100 * (total_samples - failed_validations) / total_samples if total_samples else 0\n",
    "\n",
    "        # Optional: save all predictions to CSV\n",
    "    all_predictions_file = \"all_predictions.csv\"\n",
    "    with open(all_predictions_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"query\", \"expected_response\", \"predicted_response\", \"is_exact_match\"])\n",
    "        writer.writeheader()\n",
    "        for item in all_logged_predictions:\n",
    "            writer.writerow(item)\n",
    "    print(f\"📄 Saved all predictions to {all_predictions_file}\")\n",
    "    print(f\"\\n✅ Exact Match Accuracy: {acc:.2f}%\")\n",
    "    print(f\"🛡️  Validator Pass Rate: {val_rate:.2f}%\")\n",
    "\n",
    "    # Optional: save failed cases to CSV\n",
    "    if save_errors and failed_cases:\n",
    "        with open(error_log_file, \"w\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\"query\", \"expected_response\", \"predicted_response\", \"error\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(failed_cases)\n",
    "        print(f\"📝 Saved {len(failed_cases)} failed validations to {error_log_file}\")\n",
    "\n",
    "    return {\n",
    "        \"exact_match\": acc,\n",
    "        \"validator_pass\": val_rate,\n",
    "        \"total_samples\": total_samples,\n",
    "        \"failed\": failed_cases\n",
    "    }\n",
    "\n",
    "def evaluate_exact(model, dataset, device, print_limit=5):\n",
    "    exact_matches = 0\n",
    "    total_samples = 0\n",
    "    shown = 0\n",
    "\n",
    "    for item in dataset:\n",
    "        tokens = item[\"input_ids\"]\n",
    "        sep_idx = tokens.index(SEP)\n",
    "        query = tokens[:sep_idx+1]\n",
    "        true = tokens[sep_idx+1:-1]  # remove EOS\n",
    "\n",
    "        pred = generate(model, query)\n",
    "        pred = pred[pred.index(SEP)+1:]\n",
    "        if EOS in pred:\n",
    "            pred = pred[:pred.index(EOS)]\n",
    "\n",
    "        # Exact match check\n",
    "        if pred == true:\n",
    "            exact_matches += 1\n",
    "        total_samples += 1\n",
    "\n",
    "        # Optional print\n",
    "        if shown < print_limit:\n",
    "            def tohex(x): return ''.join(f\"{b:02x}\" for b in x)\n",
    "            print(\"\\nQuery:             \", tohex(query[1:-1]))\n",
    "            print(\"True Response:     \", tohex(true))\n",
    "            print(\"Predicted Response:\", tohex(pred))\n",
    "            shown += 1\n",
    "\n",
    "    accuracy = 100 * exact_matches / total_samples if total_samples > 0 else 0\n",
    "    print(f\"\\n✅ Exact Match Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_loss(model, dataset, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')  # sum to average later\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in dataset:\n",
    "            input_ids = item[\"input_ids\"]\n",
    "            sep_idx = input_ids.index(SEP)\n",
    "\n",
    "            query = input_ids[:sep_idx+1]\n",
    "            true_response = input_ids[sep_idx+1:]  # includes EOS\n",
    "\n",
    "            for t in range(len(true_response)):\n",
    "                # model input = query + previous tokens of the response\n",
    "                model_input = query + true_response[:t]\n",
    "                x = torch.tensor([model_input], dtype=torch.long).to(device)\n",
    "                \n",
    "                logits = model(x)\n",
    "                next_logits = logits[0, -1]  # predict next token\n",
    "                next_token = true_response[t]\n",
    "\n",
    "                # Compute loss on this token\n",
    "                loss = loss_fn(next_logits.view(1, -1), torch.tensor([next_token]).to(device))\n",
    "                total_loss += loss.item()\n",
    "                total_tokens += 1\n",
    "\n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float(\"inf\")\n",
    "    print(f\"✅ Test Loss (no leakage): {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def plot_evaluation(train_loss, val_loss, accuracy, model, test_dataset, device):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(train_loss, label=\"Train Loss\")\n",
    "    plt.plot(val_loss, label=\"Validation Loss\")\n",
    "    plt.plot(accuracy, label=\"Accuracy history\" )\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print (\"Bit by Bit accuracy\")\n",
    "    evaluate(model, test_dataset, device)\n",
    "    print (\"Exact Matching\")\n",
    "    evaluate_exact(model, test_dataset, device)\n",
    "    print (\"Validator Accuracy\")\n",
    "    results = evaluate_with_validator(\n",
    "        model=model,\n",
    "        dataset=test_dataset,\n",
    "        device=device,\n",
    "        print_limit=3,\n",
    "        end_address=39,\n",
    "        save_errors=True,\n",
    "        error_log_file=\"validator_failures_vocab260.csv\"\n",
    "    )\n",
    "    test_loss= evaluate_loss(model, test_dataset, device, loss_fn=None)\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"🔢 Total Trainable Parameters: {num_params:,}\")\n",
    "\n",
    "def plot_evaluation_search (train_loss, val_loss, accuracy, model, test_dataset, device):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(train_loss, label=\"Train Loss\")\n",
    "    plt.plot(val_loss, label=\"Validation Loss\")\n",
    "    plt.plot(accuracy, label=\"Accuracy history\" )\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss per Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    evaluate_exact(model, test_dataset, device)\n",
    "    print (\"Validator Accuracy\")\n",
    "    results = evaluate_with_validator(\n",
    "        model=model,\n",
    "        dataset=test_dataset,\n",
    "        device=device,\n",
    "        print_limit=3,\n",
    "        end_address=39,\n",
    "        save_errors=True,\n",
    "        error_log_file=\"validator_failures_vocab260.csv\"\n",
    "    )\n",
    "    metrics = { \"exact_match\": results [\"exact_match\"], \"validator_pass\": results[\"val_rate\"]}\n",
    "    return metrics\n",
    "\n",
    "# Training\n",
    "def autoregressive_generate(model, input_ids, max_len=64, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    B, T = input_ids.shape\n",
    "    generated = input_ids.clone()\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        logits = model(generated)  # shape: [B, T_cur, vocab]\n",
    "        next_token_logits = logits[:, -1, :]  # take last time step\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)\n",
    "        generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return generated\n",
    "\n",
    "def train_transformer_change(model, train_dataloader, val_dataloader, hparams, device,  max_gen_len=64):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hparams[\"lr\"])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)  # ✅ Use -100 for masked labels\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    acc_history=[]\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    early_stop_patience = hparams.get(\"early_stop_patience\", 3)  # stop if no improvement after 3 epochs\n",
    "\n",
    "    for epoch in range(hparams[\"epochs\"]):\n",
    "        # -------------------- Training --------------------\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            x = batch[\"input_ids\"].to(device)\n",
    "            y = batch[\"labels\"].to(device)\n",
    "            # Safety check\n",
    "            bad_labels = y[(y != -100) & ((y < 0) | (y >= VOCAB_SIZE))]\n",
    "            if bad_labels.numel() > 0:\n",
    "                print(\"❌ Invalid label values:\", bad_labels)\n",
    "                raise ValueError(\"Some labels are outside the VOCAB_SIZE range\")\n",
    "            logits = model(x)[:, :-1, :]\n",
    "            targets = y[:, 1:]\n",
    "            loss = loss_fn(logits.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "    #------------------Validation----------------------------------------------------------------\n",
    "\n",
    "        model.eval()\n",
    "        total_tokens = 0\n",
    "        correct_tokens = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                x_full = batch[\"input_ids\"].to(device)        # includes query + SEP (+ response)\n",
    "                y_true = batch[\"labels\"].to(device)            # masked: -100 before response\n",
    "\n",
    "                # Extract only the prompt part: up to and including SEP\n",
    "                # sep_pos = (x_full == SEP).nonzero(as_tuple=True)[1].max().item()  # last SEP\n",
    "                # x_prompt = x_full[:, :sep_pos+1]               # [BOS] + query + [SEP]\n",
    "\n",
    "                # # Generate tokens\n",
    "                # generated = autoregressive_generate(model, x_prompt, max_len=max_gen_len, device=device)\n",
    "\n",
    "                # # Align with ground truth response\n",
    "                # for pred, target in zip(generated, y_true):\n",
    "                #     # Strip padding and -100\n",
    "                #     gt_response = [t.item() for t in target if t.item() != -100]\n",
    "                #     gen_response = pred[len(x_prompt[0]):len(x_prompt[0]) + len(gt_response)]\n",
    "\n",
    "                #     total_tokens += len(gt_response)\n",
    "                #     correct_tokens += sum(p == t for p, t in zip(gen_response, gt_response))\n",
    "                    \n",
    "                logits = model(x_full)[:, :-1, :]\n",
    "                targets = y_true[:, 1:]\n",
    "                loss = loss_fn(logits.reshape(-1, VOCAB_SIZE), targets.reshape(-1))\n",
    "                total_val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            best_model_state = model.state_dict()  # save best model weights\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"🕒 No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "            if epochs_without_improvement >= early_stop_patience:\n",
    "                print(\"🛑 Early stopping triggered.\")\n",
    "                break\n",
    "        scheduler.step(avg_val_loss)  # call with val loss if using ReduceLROnPlateau\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(f\"🔧 Current learning rate: {param_group['lr']}\")\n",
    "        val_loss_history.append(avg_val_loss)\n",
    "        acc = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "        acc_history.append(acc.item() if isinstance(acc, torch.Tensor) else acc)\n",
    "        print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "    model.load_state_dict(best_model_state)\n",
    "    return train_loss_history, val_loss_history, acc_history\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_hyper():\n",
    "    return {\n",
    "        \"d_model\": 128,\n",
    "        \"n_heads\": 4,\n",
    "        \"n_layers\": 4,\n",
    "        \"lr\": 5e-4,\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\": 50,\n",
    "        \"early_stop_patience\": 5\n",
    "    }\n",
    "\n",
    "def try_million_parameters():\n",
    "    return{\n",
    "    \"d_model\": 256,        # embedding size\n",
    "    \"n_heads\": 8,          # must divide d_model evenly\n",
    "    \"n_layers\": 6,         # number of Transformer blocks\n",
    "    \"lr\": 5e-4,\n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 50,\n",
    "    \"early_stop_patience\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: With Hexadecimal to Byte translation, best configuration, no weight initialization, early stop, learning rate scheduling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hparams = best_hyper()\n",
    "model = DecoderOnlyTransformer(VOCAB_SIZE, hparams[\"d_model\"], hparams[\"n_heads\"], hparams[\"n_layers\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate)\n",
    "\n",
    "train_loss, val_loss, accuracy = train_transformer_change(model, train_loader, val_loader, hparams, device, max_gen_len=64)\n",
    "plot_evaluation(train_loss, val_loss, accuracy, model, test_dataset, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c2d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: With Hexadecimal to Byte translation, Big architecture, no weight initialization, early stop, learning rate scheduling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hparams = try_million_parameters()\n",
    "model = DecoderOnlyTransformer(VOCAB_SIZE, hparams[\"d_model\"], hparams[\"n_heads\"], hparams[\"n_layers\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate)\n",
    "\n",
    "train_loss, val_loss, accuracy = train_transformer_change(model, train_loader, val_loader, hparams, device, max_gen_len=64)\n",
    "plot_evaluation(train_loss, val_loss, accuracy, model, test_dataset, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with weight Initialization\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    # vocab_size = how many distinct tokens we can embed\n",
    "    #d_model =  size of each embedding vector.\n",
    "    #n_heads: number of attention heads. Helps the model focus on different parts of the sequence simultaneously.\n",
    "    # n_layers: number of transformer layers stacked\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        #Converts each token ID into a dense vector of dimension d_model.\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD)\n",
    "        self.pos_emb = nn.Embedding(512, d_model)\n",
    "        # Even though we call it \"encoder layer\", apply causal masks later to make it autoregressive like a decoder.\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4, dropout=dropout, batch_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ]) # dim_feedforward = Size of the hidden layer inside the feed-forward network of the Transformer.\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        #Maps the output of the model (of shape [batch, seq_len, d_model]) to logits over the vocabulary.\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        self.init_weights()  # <--- call your initializer\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Token & Positional Embeddings: small normal noise\n",
    "        nn.init.normal_(self.token_emb.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        # Output layer: Xavier for balanced activations\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        nn.init.constant_(self.output.bias, 0)\n",
    "\n",
    "        # Initialize Transformer Encoder Layers\n",
    "        for layer in self.layers:\n",
    "            for name, param in layer.named_parameters():\n",
    "                if param.dim() > 1:  # weights\n",
    "                    if \"linear\" in name or \"weight\" in name:\n",
    "                        nn.init.xavier_uniform_(param)\n",
    "                elif \"bias\" in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size() # B is the batch size, T is the sequence length (number of tokens in each input)\n",
    "        positions = torch.arange(0, T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.token_emb(x) + self.pos_emb(positions)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(T).to(x.device) # Ensures the model can only attend to current and past tokens, not future ones\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=tgt_mask)  # Applies each encoder layer sequentially\n",
    "        x = self.norm(x)\n",
    "        return self.output(x) # Projects the final hidden states back to vocabulary space: for each token position, the model predicts a probability distribution over the 260 tokens (0–259)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d3b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: With Hexadecimal to Byte translation, weight initialization, early stop, learning rate scheduling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hparams = best_hyper()\n",
    "model = DecoderOnlyTransformer(VOCAB_SIZE, hparams[\"d_model\"], hparams[\"n_heads\"], hparams[\"n_layers\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate)\n",
    "\n",
    "train_loss, val_loss, accuracy = train_transformer_change(model, train_loader, val_loader, hparams, device, max_gen_len=64)\n",
    "\n",
    "plot_evaluation(train_loss, val_loss, accuracy, model, test_dataset, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: With Hexadecimal to Byte translation, big model, weight initialization, early stop, learning rate scheduling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hparams = try_million_parameters()\n",
    "model = DecoderOnlyTransformer(VOCAB_SIZE, hparams[\"d_model\"], hparams[\"n_heads\"], hparams[\"n_layers\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate)\n",
    "train_loss, val_loss, accuracy = train_transformer_change(model, train_loader, val_loader, hparams, device, max_gen_len=64)\n",
    "plot_evaluation(train_loss, val_loss, accuracy, model, test_dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fce6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No weight initialization\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    # vocab_size = how many distinct tokens we can embed\n",
    "    #d_model =  size of each embedding vector.\n",
    "    #n_heads: number of attention heads. Helps the model focus on different parts of the sequence simultaneously.\n",
    "    # n_layers: number of transformer layers stacked\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        #Converts each token ID into a dense vector of dimension d_model.\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD)\n",
    "        self.pos_emb = nn.Embedding(512, d_model)\n",
    "        # Even though we call it \"encoder layer\", apply causal masks later to make it autoregressive like a decoder.\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4, dropout=dropout, batch_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ]) # dim_feedforward = Size of the hidden layer inside the feed-forward network of the Transformer.\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        #Maps the output of the model (of shape [batch, seq_len, d_model]) to logits over the vocabulary.\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size() # B is the batch size, T is the sequence length (number of tokens in each input)\n",
    "        positions = torch.arange(0, T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.token_emb(x) + self.pos_emb(positions)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(T).to(x.device) # Ensures the model can only attend to current and past tokens, not future ones\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=tgt_mask)  # Applies each encoder layer sequentially\n",
    "        x = self.norm(x)\n",
    "        return self.output(x) # Projects the final hidden states back to vocabulary space: for each token position, the model predicts a probability distribution over the 260 tokens (0–259)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d5fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Conversion Hexadecimal Bytes to Decimal\n",
    "# Constants\n",
    "BOS, SEP, EOS, PAD = 16, 17, 18, 19\n",
    "VOCAB_SIZE = 20  # 16 hex chars + 4 special tokens\n",
    "\n",
    "def preprocess_hex_chars(example):\n",
    "    char_to_id = {c: i for i, c in enumerate(\"0123456789abcdef\")}\n",
    "    q = [char_to_id[c.lower()] for c in example[\"query\"]]\n",
    "    r = [char_to_id[c.lower()] for c in example[\"response\"]]\n",
    "    input_ids = [BOS] + q + [SEP] + r + [EOS]\n",
    "    # Create labels: PAD for query + SEP, actual IDs for response + EOS\n",
    "    labels = [PAD] * (len(q) + 2) + r + [EOS]  # PAD for BOS + query + SEP\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "with open(\"modbus_dataset.jsonl\", \"r\") as f:\n",
    "    train_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"modbus_dataset_test.jsonl\", \"r\") as f:\n",
    "    test_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"modbus_dataset_validation.jsonl\", \"r\") as f:\n",
    "    validation_data = [json.loads(line) for line in f]\n",
    "\n",
    "#turns a list of dictionaries into a Dataset object\n",
    "train_dataset = Dataset.from_list([preprocess_hex_chars(d) for d in train_data])\n",
    "test_dataset = Dataset.from_list([preprocess_hex_chars(d) for d in test_data])\n",
    "val_dataset = Dataset.from_list([preprocess_hex_chars(d) for d in validation_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45403cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1b: With no translation, best configuration, no weight initialization, early stop, learning rate scheduling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hparams = best_hyper()\n",
    "model = DecoderOnlyTransformer(VOCAB_SIZE, hparams[\"d_model\"], hparams[\"n_heads\"], hparams[\"n_layers\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate)\n",
    "\n",
    "train_loss, val_loss, accuracy = train_transformer_change(model, train_loader, val_loader, hparams, device, max_gen_len=64)\n",
    "plot_evaluation(train_loss, val_loss, accuracy, model, test_dataset, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeefa218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2b: With no translation, million configuration, no weight initialization, early stop, learning rate scheduling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hparams = try_million_parameters()\n",
    "model = DecoderOnlyTransformer(VOCAB_SIZE, hparams[\"d_model\"], hparams[\"n_heads\"], hparams[\"n_layers\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate)\n",
    "\n",
    "train_loss, val_loss, accuracy = train_transformer_change(model, train_loader, val_loader, hparams, device, max_gen_len=64)\n",
    "plot_evaluation(train_loss, val_loss, accuracy, model, test_dataset, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10ad72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model with weight Initialization\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    # vocab_size = how many distinct tokens we can embed\n",
    "    #d_model =  size of each embedding vector.\n",
    "    #n_heads: number of attention heads. Helps the model focus on different parts of the sequence simultaneously.\n",
    "    # n_layers: number of transformer layers stacked\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        #Converts each token ID into a dense vector of dimension d_model.\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=PAD)\n",
    "        self.pos_emb = nn.Embedding(512, d_model)\n",
    "        # Even though we call it \"encoder layer\", apply causal masks later to make it autoregressive like a decoder.\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=d_model, nhead=n_heads, dim_feedforward=d_model*4, dropout=dropout, batch_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ]) # dim_feedforward = Size of the hidden layer inside the feed-forward network of the Transformer.\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        #Maps the output of the model (of shape [batch, seq_len, d_model]) to logits over the vocabulary.\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "        self.init_weights()  # <--- call your initializer\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Token & Positional Embeddings: small normal noise\n",
    "        nn.init.normal_(self.token_emb.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.pos_emb.weight, mean=0.0, std=0.02)\n",
    "\n",
    "        # Output layer: Xavier for balanced activations\n",
    "        nn.init.xavier_uniform_(self.output.weight)\n",
    "        nn.init.constant_(self.output.bias, 0)\n",
    "\n",
    "        # Initialize Transformer Encoder Layers\n",
    "        for layer in self.layers:\n",
    "            for name, param in layer.named_parameters():\n",
    "                if param.dim() > 1:  # weights\n",
    "                    if \"linear\" in name or \"weight\" in name:\n",
    "                        nn.init.xavier_uniform_(param)\n",
    "                elif \"bias\" in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size() # B is the batch size, T is the sequence length (number of tokens in each input)\n",
    "        positions = torch.arange(0, T, device=x.device).unsqueeze(0).expand(B, T)\n",
    "        x = self.token_emb(x) + self.pos_emb(positions)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(T).to(x.device) # Ensures the model can only attend to current and past tokens, not future ones\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask=tgt_mask)  # Applies each encoder layer sequentially\n",
    "        x = self.norm(x)\n",
    "        return self.output(x) # Projects the final hidden states back to vocabulary space: for each token position, the model predicts a probability distribution over the 260 tokens (0–259)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f8eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3b: With no translation, best configuration,  weight initialization, early stop, learning rate scheduling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hparams = best_hyper()\n",
    "model = DecoderOnlyTransformer(VOCAB_SIZE, hparams[\"d_model\"], hparams[\"n_heads\"], hparams[\"n_layers\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate)\n",
    "\n",
    "train_loss, val_loss, accuracy = train_transformer_change(model, train_loader, val_loader, hparams, device, max_gen_len=64)\n",
    "plot_evaluation(train_loss, val_loss, accuracy, model, test_dataset, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94fd0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4b: With no translation, million configuration,  weight initialization, early stop, learning rate scheduling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hparams = try_million_parameters()\n",
    "model = DecoderOnlyTransformer(VOCAB_SIZE, hparams[\"d_model\"], hparams[\"n_heads\"], hparams[\"n_layers\"])\n",
    "train_loader = DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate)\n",
    "\n",
    "train_loss, val_loss, accuracy = train_transformer_change(model, train_loader, val_loader, hparams, device, max_gen_len=64)\n",
    "plot_evaluation(train_loss, val_loss, accuracy, model, test_dataset, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78bceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Try different configurations using weight Initialization, learning rate scheduling, early stopping, no translation during tokenization\n",
    "configs = [\n",
    "    {\"d_model\": 128, \"n_heads\": 4, \"n_layers\": 4, \"lr\": 0.0005, \"batch_size\": 16, \"epochs\": 500, \"early_stop_patience\": 10},\n",
    "    {\"d_model\": 128, \"n_heads\": 2, \"n_layers\": 4, \"lr\": 0.0001, \"batch_size\": 16, \"epochs\": 250, \"early_stop_patience\": 10},\n",
    "    {\"d_model\": 128, \"n_heads\": 4, \"n_layers\": 8, \"lr\": 0.0005, \"batch_size\": 16, \"epochs\": 100, \"early_stop_patience\": 10},\n",
    "    {\"d_model\": 128, \"n_heads\": 8, \"n_layers\": 2, \"lr\": 0.0005, \"batch_size\": 16, \"epochs\": 50, \"early_stop_patience\": 10},\n",
    "    {\"d_model\": 256, \"n_heads\": 8, \"n_layers\": 4, \"lr\": 0.0005, \"batch_size\": 16, \"epochs\": 50,  \"early_stop_patience\": 10},\n",
    "    {\"d_model\": 256, \"n_heads\": 8, \"n_layers\": 8, \"lr\": 0.0001, \"batch_size\": 16, \"epochs\": 250,  \"early_stop_patience\": 10},\n",
    "    {\"d_model\": 256, \"n_heads\": 8, \"n_layers\": 8, \"lr\": 0.0001, \"batch_size\": 16, \"epochs\": 500,  \"early_stop_patience\": 10},\n",
    "    {\"d_model\": 128, \"n_heads\": 16, \"n_layers\": 8, \"lr\": 0.0005, \"batch_size\": 16, \"epochs\": 50, \"early_stop_patience\": 10},\n",
    "    {\"d_model\": 128, \"n_heads\": 16, \"n_layers\": 4, \"lr\": 0.0005, \"batch_size\": 16, \"epochs\": 50, \"early_stop_patience\": 10},\n",
    "    {\"d_model\": 256, \"n_heads\": 8, \"n_layers\": 4, \"lr\": 0.0001, \"batch_size\": 16, \"epochs\": 50, \"early_stop_patience\": 10},\n",
    "]\n",
    "\n",
    "configs += [\n",
    "    {\"d_model\": 512, \"n_heads\": 8,  \"n_layers\": 6,  \"lr\": 0.0005, \"batch_size\": 16, \"epochs\": 100, \"early_stop_patience\": 5},\n",
    "    {\"d_model\": 512, \"n_heads\": 16, \"n_layers\": 8,  \"lr\": 0.0003, \"batch_size\": 16, \"epochs\": 100, \"early_stop_patience\": 5},\n",
    "    {\"d_model\": 512, \"n_heads\": 16, \"n_layers\": 12, \"lr\": 0.0002, \"batch_size\": 16, \"epochs\": 200, \"early_stop_patience\": 10},\n",
    "    {\"d_model\": 768, \"n_heads\": 12, \"n_layers\": 12, \"lr\": 0.0001, \"batch_size\": 8,  \"epochs\": 200, \"early_stop_patience\": 10},\n",
    "    {\"d_model\": 1024,\"n_heads\": 16, \"n_layers\": 12, \"lr\": 0.0001, \"batch_size\": 8,  \"epochs\": 250, \"early_stop_patience\": 10},\n",
    "    {\"d_model\": 1024,\"n_heads\": 16, \"n_layers\": 16, \"lr\": 0.00005,\"batch_size\": 4,  \"epochs\": 300, \"early_stop_patience\": 12},\n",
    "    {\"d_model\": 1024,\"n_heads\": 32, \"n_layers\": 16, \"lr\": 0.00003,\"batch_size\": 4,  \"epochs\": 500, \"early_stop_patience\": 20},\n",
    "    {\"d_model\": 1536,\"n_heads\": 24, \"n_layers\": 18, \"lr\": 0.00003,\"batch_size\": 4,  \"epochs\": 500, \"early_stop_patience\": 20},\n",
    "    {\"d_model\": 2048,\"n_heads\": 32, \"n_layers\": 24, \"lr\": 0.00002,\"batch_size\": 2,  \"epochs\": 500, \"early_stop_patience\": 25},\n",
    "    {\"d_model\": 2048,\"n_heads\": 32, \"n_layers\": 32, \"lr\": 0.00001,\"batch_size\": 2,  \"epochs\": 1000,\"early_stop_patience\": 30},\n",
    "    {\"d_model\": 1024,\"n_heads\": 8,  \"n_layers\": 24, \"lr\": 0.00005,\"batch_size\": 8,  \"epochs\": 500, \"early_stop_patience\": 15},\n",
    "    {\"d_model\": 1536,\"n_heads\": 16, \"n_layers\": 20, \"lr\": 0.00002,\"batch_size\": 4,  \"epochs\": 600, \"early_stop_patience\": 20},\n",
    "    {\"d_model\": 512, \"n_heads\": 8,  \"n_layers\": 16, \"lr\": 0.0002, \"batch_size\": 8,  \"epochs\": 300, \"early_stop_patience\": 8},\n",
    "    {\"d_model\": 1024,\"n_heads\": 16, \"n_layers\": 20, \"lr\": 0.00002,\"batch_size\": 4,  \"epochs\": 600, \"early_stop_patience\": 25},\n",
    "]\n",
    "\n",
    "df_configs = pd.DataFrame(configs, columns=[\"d_model\", \"n_heads\", \"n_layers\", \"lr\", \"batch_size\", \"epochs\", \"early_stop_patience\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=hparams[\"batch_size\"], shuffle=True, collate_fn=collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hparams[\"batch_size\"], shuffle=False, collate_fn=collate)\n",
    "\n",
    "results= []\n",
    "\n",
    "for idx, hparams in enumerate(configs):\n",
    "    print(f\"\\n🔁 Testing Model {idx+1} with config: {hparams}\")\n",
    "    model = DecoderOnlyTransformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        d_model=hparams[\"d_model\"],\n",
    "        n_heads=hparams[\"n_heads\"],\n",
    "        n_layers=hparams[\"n_layers\"],\n",
    "    ).to(device)\n",
    "    train_loss, val_loss, accuracy = train_transformer_change(model, train_loader, val_loader, hparams, device, max_gen_len=64)\n",
    "    values= plot_evaluation_search(train_loss, val_loss, accuracy, model, test_dataset, device)\n",
    "    # metrics = { \"exact_match\": results [\"exact_match\"], \"validator_pass\": results[\"val_rate\"]}\n",
    "    # return metrics\n",
    "\n",
    "    acc = values[\"exact_match\"]\n",
    "    validator = values[\"validator_pass\"]\n",
    "    print(f\"🧠 Exact Match: {acc:.4f}%\")\n",
    "    print(f\"🧠 Validator percentage: {validator:.4f}%\")\n",
    "\n",
    "    # Save results\n",
    "    results.append({\n",
    "        **hparams.to_dict(),\n",
    "        \"Exact Match\": acc,\n",
    "        \"Validator Percentage\": validator\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"NoTranslation_grid_search_results.csv\", index=False)\n",
    "print(\"\\n✅ Grid search complete. Results saved to grid_search_results.csv.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
